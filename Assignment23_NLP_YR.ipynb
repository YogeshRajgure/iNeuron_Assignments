{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment23_NLP_YR.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNy13fFVBkE6UwqL6kK1q7X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YogeshRajgure/iNeuron_Assignments/blob/main/Assignment23_NLP_YR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEeJ2kd0YzAw"
      },
      "source": [
        "In this assignment students have to find the frequency of words in a\n",
        "webpage. User can use urllib and BeautifulSoup to extract text from\n",
        "webpage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJAmi_m3hiON",
        "outputId": "ffb770e7-0ca5-4cdb-8a52-97c67314e169"
      },
      "source": [
        "  from bs4 import BeautifulSoup\n",
        "  import urllib.request\n",
        "  import nltk\n",
        "  import requests\n",
        "  import re  \n",
        "  from nltk.corpus import stopwords\n",
        "  from nltk.stem import WordNetLemmatizer, LancasterStemmer, SnowballStemmer\n",
        "  import string\n",
        "\n",
        "  nltk.download('stopwords')\n",
        "  nltk.download('punkt')\n",
        "  nltk.download('wordnet')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vB87pZ3ZgpmL"
      },
      "source": [
        "# enter the website on which you want to have word count\n",
        "\n",
        "url = 'https://en.wikipedia.org/wiki/Spider-Man'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEFHErshgpiz"
      },
      "source": [
        "def word_counter(url ,top_n = 30 ):\n",
        "\n",
        "  # retrive the text from given url\n",
        "  response = urllib.request.urlopen(url)\n",
        "  html = response.read()\n",
        "  raw = BeautifulSoup(html, 'html5lib').get_text()\n",
        "\n",
        "  # defining the stopwords for english so that we can remove it from word to be counted\n",
        "  stop_words = stopwords.words('english')\n",
        "  stop_words\n",
        "\n",
        "  # removing punctuation from text\n",
        "  punct = string.punctuation\n",
        "  punct = '!\\\"#$%&\\'()*+,-./:;<=>?@[\\]^_`{|}~''`` 0123456789'\n",
        "\n",
        "  lemma = WordNetLemmatizer()\n",
        "  Snowball = SnowballStemmer(\"english\")\n",
        "\n",
        "  # tokenizing text data into sentences to clean it from \\n \\t and all..\n",
        "  data = nltk.sent_tokenize(raw)\n",
        "  # convert list of sentences to a single string\n",
        "  data = ''.join(data)\n",
        "\n",
        "  clean_data = []\n",
        "\n",
        "  for word in nltk.word_tokenize(data):\n",
        "\n",
        "    if word not in punct:\n",
        "\n",
        "      if word not in stop_words:\n",
        "\n",
        "        if len(word)>1:\n",
        "\n",
        "          if not word.isnumeric():\n",
        "\n",
        "            while word.startswith(tuple(punct)):\n",
        "\n",
        "                word = word[1:]\n",
        "            \n",
        "            while word.endswith(tuple(punct)):\n",
        "\n",
        "                word = word[:-1]\n",
        "\n",
        "            if list(punct) not in list(word):\n",
        "\n",
        "              if len(word)>1:\n",
        "              \n",
        "                clean_data.append(lemma.lemmatize(word))\n",
        "                #clean_data.append(Snowball.stem(word))\n",
        "\n",
        "\n",
        "  clean_data = [word.lower() for word in clean_data ]\n",
        "\n",
        "  fdist = nltk.FreqDist(clean_data)\n",
        "\n",
        "  return [f'{word} : {frequency}' for word, frequency in fdist.most_common(top_n)]\n",
        "  #print(f'{word} : {frequency}')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVyPiYE_gpgk",
        "outputId": "3b96c48c-da48-4fe0-fb16-a08290b9988d"
      },
      "source": [
        "word_counter(url)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['spider-man : 697',\n",
              " 'the : 374',\n",
              " 'amazing : 184',\n",
              " 'marvel : 163',\n",
              " 'character : 121',\n",
              " 'peter : 115',\n",
              " 'parker : 112',\n",
              " 'man : 112',\n",
              " 'comic : 108',\n",
              " 'lee : 108',\n",
              " 'original : 98',\n",
              " 'comics : 97',\n",
              " 'new : 91',\n",
              " 'mw-parser-output : 91',\n",
              " 'retrieved : 85',\n",
              " 'avengers : 83',\n",
              " 'stan : 82',\n",
              " 'in : 76',\n",
              " 'ditko : 72',\n",
              " 'series : 64',\n",
              " 'iron : 62',\n",
              " 'black : 61',\n",
              " 'book : 60',\n",
              " 'captain : 60',\n",
              " 'steve : 58',\n",
              " 'issue : 56',\n",
              " 'doctor : 56',\n",
              " 'hulk : 56',\n",
              " 'april : 55',\n",
              " 'goblin : 52']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    }
  ]
}